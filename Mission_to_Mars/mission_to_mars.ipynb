{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99318da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843f4608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\agreg\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Headless browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome',**executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470fe146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit news site https://redplanetscience.com\n",
    "news_url = \"https://redplanetscience.com/\"\n",
    "browser.visit(news_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a0496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584c07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first article div, class list_text\n",
    "article = soup.find('div', class_='list_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2707b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the title and save in a variable\n",
    "title = article.find('div', class_='content_title').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd97e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the paragraph text and save in a variable\n",
    "paragraph = article.find('div', class_='article_teaser_body').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16e3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit Featured Space Image site https://spaceimages-mars.com\n",
    "img_url = \"https://spaceimages-mars.com/\"\n",
    "browser.visit(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88cd54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3334426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the div with the Featured Mars Image\n",
    "header = soup.find('div',class_='header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cfb6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the URL from the header used for the Featured Image\n",
    "image = header.find('img',class_ = 'headerimage fade-in')\n",
    "featured_image_url = img_url + image['src']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f5ab39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit Mars Facts site https://galaxyfacts-mars.com\n",
    "facts_url = \"https://galaxyfacts-mars.com/\"\n",
    "browser.visit(facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c13aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to scrape the planet profile table\n",
    "table = pd.read_html(facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78edf3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scraped table to a DataFrame\n",
    "df = table[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "759d5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to table, replace values with blanks \n",
    "html_table = df.to_html().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72d37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file as .html\n",
    "df.to_html('table.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ebc00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit astrogeology site\n",
    "hemi_url = \"https://marshemispheres.com/\"\n",
    "browser.visit(hemi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90c8b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for the dictionaries\n",
    "hemisphere_image_urls = []\n",
    "titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa78a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5de68fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on links to go to the hemisphere pages\n",
    "links = []\n",
    "area = soup.find_all('a', class_ = 'itemLink')\n",
    "\n",
    "for each in area:\n",
    "    try:\n",
    "        link = each.get('href')\n",
    "        if link not in links:\n",
    "            links.append(link)\n",
    "        browser.visit(hemi_url + link)\n",
    "        # Parser\n",
    "        html2 = browser.html\n",
    "        soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "        # Scrape links to images\n",
    "        downloads = soup2.find('div', class_ = 'downloads')\n",
    "        anchor = downloads.a\n",
    "        href = anchor.get('href')\n",
    "        # Scrape h2 tags to get titles\n",
    "        title = soup2.find('div', class_ = 'cover')\n",
    "        name = title.h2.text\n",
    "        # If image link not in the list, append\n",
    "        if hemi_url + href not in hemisphere_image_urls:\n",
    "            hemisphere_image_urls.append(hemi_url + href)\n",
    "        # If title not in the list, append\n",
    "        if name not in titles:\n",
    "            titles.append(name)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f25425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5e76e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
